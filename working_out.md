# Working Out
## A simple guide to Machine Learning

Software Developers write programs that use Control Flow Structures, Data Structures, and Functions, and these programs can usually process more information than a human can and at much faster speeds. In fact, the volume of data only affects the choice of algorithm when the program becomes too slow, but in essence the way the data is processed is still comprehensible by a person. These programs fall into the category of procedural programming, where each procedure is laid out during coding. Declarative programming flips this over and instead requires developers to lay out the inputs and how the outputs should look like, and it is the program that decides how to get from one to the other. The most common example is SQL, where you define what tables a query has access to, and what attributes the output has (sorted, inserted into table, limit to 10 elements). *Machine Learning is another technique for declarative programming.*

There are often times that you have input data and some expected data for a program, but you are unable to determine what code to write. In a procedural language, it means you can't comprehend what procedures need to be performed. In a declarative language, it usually means the language does not have a way to express the answer, or the way to express it is also incomprehensible. The next logical step could be to train a machine learning model to determine on its own how to solve the problem by showing it inputs and outputs and letting it connect the dots. There are many resources, papers, and videos, that explain why machine learning techniques are able to learn techniques we never thought of, and see patterns we never could. Arguably the best technique, neural networks, can learn practically anything as long as the network is designed correctly. The key to their success, regardless of the algorithms they use to learn, comes from their lack of *human bias.* We as humans solve problems the way we do because our entire line of thinking is shaped by being a human and acting like a human. As such, we can only ever solve problems the way a human is expected to. A machine learning model may be designed by humans, but their paradigm of thinking is so basic and naive compared to the human brain that it's not held back by our way of thinking for problems that require a totally different style of solving.

In this guide, I'll primarily discuss how to think about neural networks, and barely touch on the other techniques of machine learning. However, you can apply this lens of thinking to these other techniques to gain useful insight as well.

## How to build a neural network

Nowadays, neural networks usually refer to deep neural networks; They're simply a lot smarter. A neural network generally consists of a sequence of layers, and each layer represents a mathematical function applied on the output of the previous layer; The input layer is a function on the input. Each function has a set of *learnable* parameters, and these parameters evolve during training to change how the function works. The most famous function, the linear function, has $(I \times O) \times O$ parameters where $I$ is the size of the previous output, and $O$ the size of the output of the linear layer. But that expression isn't really intuitive. What is a linear layer doing? Well, in a linear layer there is a preset number of neurons; One for each number that is supposed to be produced. If we made a linear layer that outputs 4 numbers, the layer has 4 neurons. Each neuron is "connected" to every number in the input. This connection is just the number in the input multiplied by a weight, and each number in the input has their own weight *per* neuron. Lastly, after summing all these input-weight products, we add a single learnable bias. It is just a constant, but it is not always present. This is why the layer is called "linear", it's only composed of linear functions, and these linear functions only operate on real numbers. Since most neural networks have linear layers, most neural networks can only operate on real numbers.

Here's the catch, if you combine a bunch of linear layers in sequence you essentially only have 1 complex *linear* layer. If you know your inputs and expected outputs are linearly correlated this is fine. However, most problems are non-linear. This means the model will not be able to learn an appropriate way to solve these problems. How do you make it non-linear? You use activation functions. Activation functions are always non-linear, and they are normally placed after each layer. Most activation functions don't learn anything; Their sole purpose is to infect the model with non-linearity. The simplest and most popular activation function is the Rectified Linear Unit (ReLU), which just clamps the input to $[0, \infty)$. That's it. Nevertheless, it is all it is needed for a model to become non-linear and learn complex problems. The usual explanation for what an ReLU *really* adds is it gives the model the ability to disable parts of itself as data flows through. Both negative and positive inputs activate a neuron in a linear layer, but values close to 0 weaken the neuron. With an ReLU, all negative values become 0 thus disabling all neurons that primarily rely on that earlier neuron (the largest weight is for that disabled neuron).

In essence, the process of designing a model involves stringing together enough functions such that the resulting model has enough *complexity* to solve the problem at hand. Just from that, you might think there's a lot of experimenting as opposed to the conciseness of regular programming. There are, however, numerous insights you can draw while designing a model that can guide its direction towards solving a problem.

## So what does it really do?

We can visualize the input space of a model as being a very high dimensional space. Let's say we are developing an image model that takes in 28 pixel square RGB images. That's $28 \times 28 \times 3 = 2352$ dimensions, where each point in this space is a valid image. However, that's misleading as that is the space of *all possible images* of that size, which includes all the noisy garbage. If our problem is to classify handwritten digits, not all these points will ever be accessed. One of the things that happens if you were to train a model on data like this is the model learns the true dimensionality of the input space and finds a mapping from the naive space to the true space. Then from this true space the model learns another mapping to the output space. The combination of both mappings is what solves the problem of classifying digits.

That all being said, the field of machine interpretability is still growing. Not every model has been explained yet. While it is usually assumed that the above explanation is what is occurring, it is not proven to always be the case. At the very least, the training algorithm we use doesn't even express that line of thinking at all; It's a commonly learned behavior, much like how the same behaviors can be observed across animals evolving independently around the world. Another issue is that there is no guarantee that these two steps happen cleanly; The model could be doing both simultaneously, and at different rates for each portion of the input.

In the case of classifying handwritten digits, the number of dimensions in the output space is 10. One dimension corresponds to one digit, and the magnitude of the output in a dimension corresponds to the likelihood of that dimension/digit being the correct answer. Now you might ask, "What is the dimensionality of the true input space?" That is a whole mess on its own. First of all, there is no guarantee that the model will find the most efficient line of thinking when solving a problem. One model might see that the dimensionality is 11, while another sees 26. However, surely the smallest dimensionality should be 10 right? After all, that is the output dimensionality. This is a common misconception. The main attribute of a well-formed, true input space is that it is almost perfectly reversible. If the model learns how to convert from the naive space to the true space, there should be a way to convert back from the true space to the naive space. What this ends up looking like is a machine learning based compression, and this technique can often beat general compression algorithms. The catch, however, is that these models can only compress data similar to what they are trained for. Regardless, the dimensionality of the true input space is irrespective of the output space. They are two separate things. The former is concerned with what actually needs to be processed, while the latter is what actually needs to be produced.

We'll discuss more about the applications of this compression, but it should be noted that the "intelligence" and "usefulness" of a model is correlated with its ability to compress the input space. Large Language Models are sometimes compared by their ability to remember large texts within a fixed context window.

## The Machine Bias

A neural network may not inherit human bias, but it most certainly gains biases from the way it is designed. This has profound implications on what the model learns and how it accomplishes a task. Every function you add to a model introduces a bias, and this is intentional. If the true problem space is a small fraction of the actual problem space, we want to bias our model towards finding this true problem space. If so, what bias does a linear layer add? A linear layer, at the beginning of training, assigns a roughly equal priority to all inputs. This biases the layer to treating all inputs as equally important. In the context of digit classification, that's not a good bias as many pixels are empty. Sure, the empty space inside a 0, 8, and 9, is important, but the pixels around the digit up to the edge are not. What bias does an ReLU add? Well, an ReLU implies that there should be different modes of thinking in the model to solve the task. We'll see more of what this looks like later.

Poor biasing can prevent a model from learning the correct approach, but it usually just prolongs the training. It is entirely possible to write a handwritten digit classifier made entirely of linear layers and ReLUs, but the number of learnable parameters will be very high for the actual task at hand. A better layer to use for spatial data, such as images, is the Convolutional Layer. To put it simply, a convolutional layer is just a tiny linear layer (when compared to the total input) that slides across the input. This is the first layer we are discussing that has dimensionality. ReLUs operate element wise, and linear layers are 1 dimensional. Convolutional Layers can have any number of dimensions, and these dimensions match the *rank* of the input data. Even though the input is $28\times28\times3$, the data has a rank of 3. There are three axes. In such a scenario, we would use a 2D Convolutional Layer. The third dimension comes from the input channels of the Convolutional Layer, which coincidentally tends to the channels of the RGB image. In a linear layer we provide input and output size, and in a convolutional layer we provide input and output channels, and kernel size. There are other parameters involving how the layer slides across the input, but we will ignore them for now even though they are important in practice (it affects the final output size).

Each output channel can be thought of as a filter. If a Conv of kernel size $5\times5$ outputs 3 channels, it is really just 3 filters of $5\times5$ pixels. Each filter is connected to each input channel in the same way that each output neuron in a linear layer is connected to each input value. The innovation that came from Convolutional Layers is that they are limited in what they could see in the whole input data. If a Conv is only $5\times5$, it can only ever see 25 pixels at once, and all 25 pixels are ingested in the same format that they were given in; If a pixel A exists next to pixel B, then pixel A will be processed with pixel B by its side, provided they both fit in the kernel. In other words, Convolutional Layers are biased towards finding local features, as opposed to linear layers finding global features.

This bias is so useful as it causes the model to slowly find important *features* over and over again, and the usual continuation after a sequence of Convolutional layers is to send the processed data into linear layers. The idea is that the input is progressively transformed into an encoding of all the features in the input, which the linear layer can then look at as a whole to make decisions. As such, the usual recommendation when working with data that has many local features is to use convolutional layers.

The most important thing to remember is that biases can come from *anywhere*. Revisiting the task of classifying handwritten digits, what is the implication of setting the output space to 10? Why not just one dimension, and have the value 1.0 correspond to 1, and 2.0 correspond to 2, etc.? By doing it this way, we are suggesting that there is spatial locality between neighboring digits; That the number 1 *looks similar* to the number 2. If the model suggests 1.5, it's trying to tell us that it is debating between 1 and 2. This is absolutely detrimental as these digits don't look anything alike, and trying to form connections this way will just hinder the model's learning. While you could of course argue about some similarities between all the digits, it is simply cleaner and clearer when you assign a digit to a dimension.

## So how does it actually learn?

The key technique that is used to teach these neural networks is back-propagation. There are many useful visualizations and technical explanations about it, so I will only explain the idea behind it. Think back to your calculus class, how do you find the minimum of a function? Well, both minima and maxima share the same property: the gradient at that point is 0. Just differentiate the function and solve for 0. Great, now what if the function is too difficult to differentiate? You can solve it iteratively, provided you can find the derivative at a point. It's easy enough with just one dependent variable, $x$. Just pick any random point and differentiate. If the derivative is positive, step towards negative. If the derivative is negative, step towards positive. If the gradient is large, take bigger steps. Somehow, without knowing the function you are still able to traverse it towards a minimum. However, you should keep in mind that this is not guaranteed to find the global minimum. If you fall in a valley of a local minimum you might never make it out. Ominous, but allegedly not common in practice as we'll see.

What does this insight lead to? Looking back at the premise for all this, we have a lot of input data and corresponding expected data. In the case of digits, we have many images of handwritten digits with correct labels, but no idea of what program connects the two. From a mathematicians' perspective, we do not know the function that maps from images to digits. However, we have a neural network which is essentially a bunch of nested functions. If we could quantify how wrong the neural network's output is, we can pull off something amazing, but how do we do that? This is a job for the loss function. These are functions that quantify how different the actual output is from the expected output, with the distinct advantage of being differentiable. These functions are usually opaque and I couldn't figure out any helpful analogy for how these were created, so I'll just pick the Cross Entropy loss function and move on. This function only works if the actual and expected data are between 0 and 1 exclusive, and it is a particularly aggressive loss function that penalizes heavily for being wrong. By attaching this last function onto our neural network, we can now rephrase our question of how do we classify digits to *how do we minimize the loss of this function?*

You might think that the dependent variable is the input data, but that is not what we want. We want the model to learn how to solve the problem. Instead, the input data is the constant and the parameters are all the dependent variables that we tweak to minimize the loss. For each input data in the sequence, we differentiate the model and find the gradients for each parameter. We then move along these gradients by a tiny amount and move on to the next input. We do so in tiny steps so that the model finds the best parameters in a *controlled* way. If the steps are too big, we might dance around the minimum. Furthermore, each input data essentially creates a unique function with the model, so finding the global minimum of any given input data wouldn't really help with finding the global minimum of the whole dataset. Even worse, the global minimum of the dataset may not be the global minimum of the whole theoretical problem set. It is a given that you would not have all of the possible data your model would ever work with in a database, as it would just be easier to look up the answer in the database than to train a model to find the answer. As such, your dataset has to be an unbiased representative slice of the theoretical complete dataset for your model to have the best chance at finding the global minimum for the whole problem.

This is also the reason why the loss can't be an arbitrary number that you make up. You can't just write an if statement that returns 100 if wrong and 0 otherwise. The loss function has to be differentiable to allow the loss to be correlated to the correct parameters. Similarly, the magnitude of the loss function affects the rate of learning, but not what is learned. If we use two different loss functions for the same model and they both output the same loss, the two models will still learn different things because the derivative of the loss functions are different, meaning different parameters are important.

If you thought that the input space of just these handwritten digits is too large to comprehend, the space in which the model's loss lives in is even bigger. Given a model with 100,000 parameters, it will also have that many dimensions in the loss space. What arises is a positively complex loss landscape with many peaks and valleys, and the hope is that for any given problem there are valleys deep enough to correlate to a good solution, and that the model can find it. There is a lot of advice about how to avoid local minima, but in practice it seemingly doesn't happen all that often. If we imagine a 2D chart of a line corresponding to loss, it's very easy to see how we can get stuck in a valley. The only escape is to jump out. Now extend that into 3D. It's a surface with a valley, but are you still stuck? You would have to be stuck along two dimensions at once. In other words, if in front and behind of you is a steep wall, what about to your right or your left? If you extend this analogy all the way up to the number of parameters, it's easy to see how unlikely it is for your model to be stuck, but that could be wishful thinking to generalize to all models.

Given the gradients, how should we tweak each parameter? This is a complicated problem involving optimizers, and the first optimizer created is stochastic gradient descent. Regardless, all optimizers take in the gradients along with a learning rate. This learning rate is used to change how strongly the optimizer should follow the gradients, and a small learning rate will result in slow learning but allows the model to settle into the best solution, while a big learning rate leads to fast learning but potential noise as the model jumps around the best solution. The optimal way is to modulate the learning rate with a learning rate scheduler. Even something as simple as a learning rate the reduces over time allows the best of both worlds with fast learning at the start followed by precise learning at the end. It is usually the case that even after providing the whole dataset the model has not dropped enough in loss yet. No worries, just feed the dataset again and continue. This is an epoch, and during training multiple epochs may be performed. An epoch is a hyperparameter, which is just a parameter that is separate from the learnable parameters. The other important hyperparameter is batch size. You can feed a whole batch of data at once and perform one learning step from it, and it's often better as it allows the optimizer to guide the model towards the minimum of at least several inputs at once. It is also more efficient to use a batch size that fits neatly in your GPU.

## So why doesn't it learn?

There are two main concerns during training, overfitting and underfitting. Overfitting means that your model minimized the loss by *memorizing* the correct outputs for most inputs, and this usually happens with models that are much larger than the available data, and trained for too many epochs. Underfitting just means your model can't do anything right. The general advice is to overfit and shrink, but that is up to you. If a model overfits the data, it means that it found the global minimum for the dataset, but not the theoretical. The way this is tested is by having a training dataset and a testing dataset. When checking for overfitting, we use the testing dataset which it is not trained for and examine how its loss compares. A testing loss that is much higher than in training suggests overfitting. Underfitting just manifests as a high loss even in training.

There are many reasons the model might be underfitting, with the least glamorous of all being bugs in the model itself. Perhaps you configured 10 layers but only ever use 3 because of a typo, or maybe you've put an activation function on the output that prevents the model from producing all the numbers that it should be producing. It's always worth checking. The other reasons include a lack of complexity. If the task is difficult, it necessitates a larger and deeper model. On top of this, if the model is biased in the wrong direction it may outright refuse to learn anything useful. Training a model for too long is a form of bias as you overly-emphasize the training dataset, leading to overfitting. Training should end just as the model's loss plateaus.

## The deeper we go

It has been shown that deeper models learn better than fatter and shallower models with the same number of parameters. If that's the case, what's stopping ultra-deep models from appearing? One of the main issues is the exploding gradients problem. Once again, there are technical analyses of these phenomena, but the analogy is the Butterfly Effect. A model is inherently a chaotic system as each output neuron depends on every other neuron before it. If we change the input data slightly, even for just one neuron, so many other neurons rely on that data, leading to a cascading change across the model. This appears to us as a very large gradient which worsens with depth. These make the life of optimizers tricky as the correct amount to move by can be disproportionately affected. While there are techniques such as gradient clipping, the best course of action is always to evaluate if depth is needed. Sometimes, it is an essential component.

When dealing with sequential data, we can pass the whole sequence to the model, but that will result in a very large model that will not learn well. The bias in this case is that every element in the sequence is equally important. In many scenarios, the older an element the less important it is, or at the very least the less accurate we need to remember. Instead, we can teach the model to produce an intermediate representation that we feed into the next invocation with the next piece of data. In doing so we've designed a recurrent neural network, but yet again we suffer from the issue of depth. To train the model to produce this intermediate representation, we provide batches of sequences and run the model across each sequence. If a batch has 256 elements with a sequence length of 10, we'll actually run the model 2560 times. The apparent effect is the model appears to be much deeper without having 10 times as many parameters. Not good if we wanted longer sequences.

Instead, we need smarter layers, such as LSTMs and GRUs. On top of fulfilling the normal duties of a linear layer, these layers learn how to forget, thus preventing gradients from growing too fast. As a result, these can handle much longer sequences. In recent times, we've seen the introduction of the transformer, which uses attention to look through the sequence and determine what is relevant. That is out of the scope of this guide however.

## Honorable Mentions

There are many components that you can expect to lean on when developing a neural network. Armed with the lens I've prepared you with, I will just list them out here:

- Normalization - A form of regularization, which guides the model to generalization (finding the global minimum of the theoretical dataset). Very useful.
- GeLUs - A fully differentiable form of the ReLU, but more expensive. Used extensively in ChatGPT (at least from the papers they used to release).
- Sigmoid and Tanh - Activation functions that bound the output to $(0, 1)$ and $(-1, 1)$ respectively. The effects of using these as opposed to ReLUs is not obvious, and you should just try them out to see if that helps.
- Auto Encoders - These are models that only learn the first transformation from the naive input space to the true input space. When fully trained, other models can be attached to shorten training as the extension only has to learn the last transformation. While useful for compression, the range of things an auto encoder can compress is limited to what it is trained for. For many small problems, it is just more straightforward to train a full model straight away instead of first training the auto encoder. They can be useful to estimate the ballpark of how large a model for the dataset should be.

## Summary

The field of machine learning is very much an experimental field, but with some very pleasing analyses we can make informed decisions to guide our models towards performing well. Even with all of this guidance, it is always worth checking all configurations that you can, even if it goes against common advice. No one can fault you for creating a working model that contradicts what we know; You would just be applauded for it.
