# Working Out
## A simple guide to Machine Learning with Neural Networks

*This guide is best read from a Markdown editor that supports inline LaTeX. Examples are VSCode and Obsidian.*

Machine learning algorithms, by and large, involve a computer program learning from examples of completed tasks. If the problem is classifying handwritten digits (determining what digit is in the image), you can manually perform the task on some samples and provide what you've done to one of these algorithms, and they will be able to connect the dots between the input and output to determine, in its own way, what decisions had to be made. The idea is that not only will these models learn what you did to the training examples, but also what could be done on all future inputs that you give it, which in turn means that the model has found a general solution.

In recent times, one machine learning technique has triumphed above others and that is deep neural network. These networks are composed of layers, with the input flowing into the first layer and the outputs of each layer flowing into the next one until we get one final answer. As we will see, these layers come in many forms, and stringing together the right combination of layers for a given problem will lead to an optimal solution. That of course begs the question of how to determine what the best combination is? The most comprehensive answer is to determine through experimentation, but there is a lot of theory that can be applied. Through my brief experience so far I have thought of some helpful analogies that I will use to explain, from the ground up, how neural networks work.

## So what are these layers?

For reasons I will explain soon, the layers of a neural network are really just mathematical functions that can be differentiated in most of its input space. Say we have a function $f$ with dependent variable $x$. When we find the derivative of the function at a specific point, say $x_1$, the resulting value is the gradient of function $f$ at that point. This gradient tells us how the values of $f$ change around $x_1$. This can be quite helpful if we want to navigate across values of $f$ but it's too complicated to perform any meaningful analysis.

Analyzing the derivative is the main goal in training, so this requires that all layers are mostly differentiable. Essentially, this means that each layer is a function, and a neural networking model is just a bunch of nested functions as one layer feeds into the next. Each layer has a set of learnable parameters, and it is these values that evolve across training. They are also the most commonly expressed metric when describing these models. For example, all flagship LLM models are rumored to have at least 1 trillion learnable parameters. However, the largest LLM model you can run on your gaming PC to almost the same degree of accuracy as the researchers who made it is a model with only 8 billion parameters.

The most basic layer which is also present in almost all models is the linear layer which operates on a 1 dimensional array of real numbers. To define one, we simply provide the input length and output length. For each number that is supposed to be output, the layer calculates the value by multiplying each input number with a unique learnable weight before summing them all up and adding another optional learnable value called the bias. This means that the number of learnable parameters is $\text{Input Length} \times \text{Output Length} + \text{Output Length}$. When this layer was first invented, each output value was imagined to be a neuron that was fully connected to all input neurons, thus spawning the name Neural Network. This is also why this layer is also called Dense, or Fully Connected.

By chaining together layers of linear layers, you can build up a complex model with many interconnections that theoretically allow the model to see all sorts of patterns between all the numbers you give it. Unfortunately, if you combine linear functions together, the resulting function is also linear. This makes it impossible for the model to learn non-linear problem-solving, which is what most problems require. This is where activation functions come in. These are strictly non-linear functions that are usually placed after each layer. These infect the model with non-linearity throughout the layers so that each layer can perform a non-linear operation on its inputs. The most commonly used activation function is an ReLU which simply clamps each element in the input to $[0, \infty)$. In other words, it replaces all negatives with 0. Other examples include the Logistic function, Tanh, GeLU, and the leaky ReLU. The hope is that with enough non-linear layers, the model will learn a complicated set of procedures that is sufficient enough to solve the problem.

Another common layer is the Convolutional Layer. To describe one, we provide the number of input and output channels, and the kernel size. This layer can be visualized as a set of filters, one for each output channel, and each filter follows the kernel size. While the linear layer is strictly 1 dimensional, the Convolutional layer can take up any dimension. As an example, we may define a Conv2D layer as having 3 input channels, 8 output channels, and a kernel size of $5 \times 5$. Each pixel in an output filter is calculated very similarly to a linear neuron; You multiply the value of each pixel in each input filter with a unique learnable weight, add them all together, then add a learnable bias. This layer is then slid across the whole input to produce the output. These layers are very often used for 1D, 2D, and 3D data, but the approach is applicable to all dimensions.

## So why the variety of layers?

Technically, it is possible for many problems to be solved by transforming the input data into real numbers, reshaping into a 1D array, and piping it through a model of only linear layers. If you were to try this out, you might find some success but generally you will suffer from long training times and poor performance. Why is that the case?

Instead, we should be compelled to ask why a well-designed model works well in the first place? The answer, in fact, lies in our flaws. When we solve problems, we solve them the human way because our brains are human brains shaped by human experiences; We have a human bias, and sometimes that makes it difficult for us to solve some problems. My favorite example is the Curse of Dimensionality, which is the observation that certain behaviors found in some dimensions don't generalize to all dimensions. The most understandable example is that of knots; You can only tie a rope into a knot in 3D space. In all other dimensions, there is too much freedom. Can you visualize that? Of course not. Your human brain is bounded to thinking in terms of 2D or 3D space.

In contrast, machine learning models are defined with simple functions chained together. They bear no real resemblance to a human brain and many people believe calling neural networks that name is misleading as real neurons do not function that way. By picking specific layers, we can shape the way the model *might* think, thus *biasing* the model towards the right line of thinking. This type of bias is called inductive bias, and is different from dataset bias (having a lot of right or left leaning data for example). This inductive bias is the most powerful aspect of neural networks as it is so easy to change it, and mastery of this bias allows us to write models that learn faster and generalize better to unseen inputs.

Let's quantify this bias. What bias does a linear layer add? At the beginning of training, all weights and biases in a linear layer are random but similar. This means that each neuron approximately favors all of its inputs, which leads to the linear layer being biased towards finding *global* features. As a quick aside, it is worth discussing what features are. A feature is a distinct and measurable property that can be found in a dataset. As humans, we tend to identify very concrete features, such as a person's gender or skin color. When we look at handwritten digits, we look for visual features to distinguish them. An 8 has two loops, but 0 and 6 and 9 have one. A 1 is just a line, and a 9 is a circle and a line. Are these features global features? I would say no. If the task is identifying digits, the digit itself is the global feature. By using only linear layers, we are essentially throwing our model into the deep end. It believes that there is a way to interpret the whole input to determine the digit, and in all likelihood the model will eventually learn the correct steps, but this is a suboptimal approach.

A better approach would be to find smaller features in the image and determine what any given combination of features means. This is where Conv2D comes in. Convolutions are commonly used for spatial data such as images as they are biased towards finding *local* features. These are features that take up a small portion of the input and involve data that is close together. In an image, there is little sense in comparing pixels that are far apart as clusters of pixels tend to carry more meaning. A Convolutional layer is forced to find these as the layer can only ever see a fixed number of data points at a time. Given this ability, what might Conv2D's find in an image of a handwritten image? For simple tasks like these, it has been observed that there are some patterns that these Conv2D's look for. If a common layout for classifying images is followed &mdash; several Conv2Ds into several linear layers &mdash; the first Conv2D tends to evolve into an edge detection layer. The subsequent layers then look for larger features (remember that each Conv2D downsizes the input, so the same kernel size will be able to see more the deeper we go). One filter of one layer might look for a vertical line, while another a diagonal line, and then another filter in another layer will look for crosses, and another circles.

Keep in mind that the presence of these features is only ever represented with real numbers; The model learns its own way of keeping track of these features. When sufficient local features are extracted, we feed all of them to the linear layers. It makes sense to use them now as we want to look at the combination of all features across the whole input to determine what digit is in the image. While this is a rosy image of what happens, the actual behavior is unpredictable. A model may find features we don't even understand, and yet still function perfectly well. If the first layer usually performs edge detection, should I preprocess images with edge detection? Maybe. The answer, as always, is to experiment.

## How big should these layers be?

To understand how large a model should be to solve a task is a difficult problem that has not been fully solved. The best way is to start large and shrink as needed, but what is the actual pattern behind the sizes of these models? For an in depth analysis, you can research the neural scaling laws which attacks this very idea, but in summary we need to think about the dimensionality of our data. While a linear layer operates on a 1D array of real numbers, it is more helpful to think of each array representing a point in space. As a result, each element of the array is the distance along a certain dimension. If a linear layer accepts 10 inputs, it really is operating on a 10D point.

Our handwritten digits come in images of $28 \times 28 \times 3$, where 3 represents the RGB channels. This means that each image is a point in 2352 dimensional space! This is of course misleading as not all of this space is populated. While each color channel is represented by a real number which lives in $(-\infty, \infty)$, we usually normalize colors to $[0, 1]$. Furthermore, this input space includes all images which are just noise as well, which we can assume is not necessary (let's imagine we use expensive scanners to collect these images). The first layer of the model will need to accept the raw 2352 dimensional point because that is just what these images come in, but the model should be designed to quickly squeeze down this dimensionality, preferably with more convolutions. Since there are 10 digits to classify, the output dimensionality is 10. This means there will be 10 output neurons in the last linear layer, and the job of the whole model is to squeeze out the correct data while compressing the dimensionality from 2352 to just 10.

What about in between? How many layers should there be, and how big should they be? While I have found some attempts at explaining what all models do in common, I have since learned that those simplified analogies do not really help with informing developers on how big these layers should be. The real answer is that each layer needs to have enough dimensions to perform the role that layer is assigned, but we do not assign roles to each layer as that is learned autonomously. If we give a layer too little dimensions, another layer might pick up the slack. This fundamentally confounds how we think about shaping these layers, and as a result the best and only real answer to determining how big these layers should be, outside of some very complicated mathematical analyses, is to just experiment. Start big and shrink down.

Fortunately, we at least know that the input layer must be able to handle the dimensionality of the input, and the output layer has to match the dimensionality of the output. You may ask then, why a dimension for each digit? Why not just one dimension, and have the value 1.0 correspond to 1, and 2.0 correspond to 2, etc.? By doing it this way, we are suggesting that there is spatial locality between neighboring digits; That the number 1 *looks similar* to the number 2. If the model suggests 1.5, it's trying to tell us that it is debating between 1 and 2, and that there is no way to debate between 1 and 9 for example. This is an absolutely detrimental bias as these digits don't look anything alike, and trying to form connections this way will just hinder the model's learning. While you could of course argue about some similarities between all the digits, it is simply cleaner and clearer when you assign a digit to a dimension. From a statistics standpoint, digits are categorical data, not ordinal.

## How does it learn?

The general idea in machine learning is to give the model some input and compare the actual output to the expected. Based on the differences, you tweak the model, and then you repeat. As such, there needs to be a value that you can calculate that can quantify this difference/error. With neural networks, this is called the loss, and it really is just a single real number. It is almost always positive, and a larger loss means higher difference or more wrongness. The sensible question to ask now is how do you use this one number to determine how to tweak the model? That is where we return to derivatives.

If our loss function is also differentiable we can chain it onto our model and produce a function that is not only differentiable, but also only ever produces a single real number. Remember, neural networks are designed to accommodate all sorts of data. Our digit classifier model we've been discussing outputs 10D data, so boiling the performance down to just a single scalar is crucial. Now, instead of asking how we can produce a model that can classify digits, we can instead ask how do we minimize the loss of this function we just created?

Calling back to Calculus class, we know that the minimum of a function has a gradient of zero. If we treat the gradient like gravity, we can follow this gradient down like rolling down a hill. When we bottom out, we know we've reached a minimum. This is known as gradient descent. Although, you might remember that many functions have many minima, some higher than others. We want to find the global minimum as this represents the limit of how well the model can solve the problem. If the model has reached the global minimum in loss, it is mathematically impossible for there to be a better solution with the same architecture (layers, activation functions, etc.).

It might be tempting to differentiate our model and perform gradient descent to optimize it, but that would be counter-productive. Right now, our model is a function whose dependent variable is the *input*. Performing gradient descent on that would result in us finding the input that results in the lowest loss, instead of finding the model that produces the lowest loss. No, instead we treat the input as a constant, and the learnable parameters as the dependent variable. For the `gym-burn` crate, the image auto-encoder model has around 100,000 parameters. Indeed, this does mean that the function we're differentiating has 100,000 dependent variables. When we are finding the gradient of this function, we are finding 100,000 gradients, each one pointing some random direction. Once obtaining these gradients, we update each parameter slightly according to these gradients and repeat.

It's easy enough to find the minimum of a function with only one dependent variable. Just plot it out in Desmos and look. To plot a 100,000 dimensional function we need 100,001 dimensional space. There is no real way to analytically determine the minimum of such a complex loss landscape, so that is why training is performed iteratively with small steps. These steps scale with the gradient, but they are always scaled down small as there is no telling how small of a hole the global minimum is in. With steps that are too large, we might jump around the cusp of the global minimum. That being said, there is a benefit to taking large steps as it can help us jump out of local minima and saddle points.

As a quick aside, it is important to note that most well-formed and non-buggy models should converge on something. It is quite implausible for a model to be so deeply stuck in a local minimum. Imagine, if you will, Mario on a 1 dimensional level. To trap Mario, we need to block the left and right walls. In a 2D level, we need to block the top and bottom as well. In 3D we need to block the front and back. In 100,000 dimensional space, the model needs to be trapped in all 100,000 dimensions to be truly stuck. Instead, the main danger of local minima and saddle points are the small gradients. These are instances where we need energy to jump out and away, but the shallow gradients trick the model into staying put.

The learning technique I'm describing is Stochastic Gradient Descent, and it is still in use today. Its role is the optimizer, and there are better optimizers for specific tasks. Right now the usual recommendation is the Adam optimizer, but yet again I encourage you to experiment. These optimizers use different strategies for maneuvering in this loss landscape in search of the global minimum, but even after feeding the whole dataset into the model that might not happen. As mentioned before, to keep training stable we keep steps small. This scaling factor is called the learning rate, and it can be on the order of $10^{-4}$ or lower. While our model might be making good leeway in reducing its loss, even after digesting the whole dataset it might still be underfitting (i.e. underperforming). No worries, just feed the dataset again. A single instance of this is called an epoch, and we typically train with tens of epochs or longer. The number of epochs, and the learning rate, are hyperparameters, which is to say that they are parameters that are not learned; We the developers pick it.

There is a disadvantage from using many epochs; It might bias the model towards overly prioritizing the dataset. Remember, we want to find the global minimum of the *problem* we are trying to solve. There is a theoretical complete dataset of all $28\times28\times3$ images that contain handwritten digits, but we only have a small slice of that. We always design our dataset to be a representative subset of the whole theoretical dataset in the hopes that the global minimum of the training dataset coincides with the global minimum of the theoretical dataset. Overly prioritizing the dataset can lead to overfitting, where the model essentially memorizes the training dataset and answers, and just regurgitates them during training. This is diagnosed with a testing dataset, which the model does not learn from. If the model performs very well in training but poorly in testing, that is a symptom of overfitting. However, it is only possible for a model to memorize the training dataset if it is much bigger than the dataset. Small models with very large datasets are at a distinct advantage.

Another important hyperparameter is batch size. Instead of finding the gradients for one input, it is beneficial to find the gradients of a batch of inputs. If the batch is an approximate slice of the whole dataset, the model might have a better chance at finding the global minimum. This is akin to solving multiple questions at the same time, instead of understanding one question very well and realizing that what you learned doesn't help with the next question. Not obeying this requirement of the batch being approximately the same distribution as the whole dataset is disastrous as I've come to learn, and it can truly prevent your model from learning anything. Don't be like me. Shuffle your datasets! Even after each epoch!

If you are struggling to pick a learning rate, you can use a learning rate scheduler to interpolate across values throughout training. Early on, a model can be given a large learning rate to get a headstart, and once we're nearing a low loss we can pick a lower learning rate to fully settle into the minimum. Interestingly enough, in the landmark paper "Attention is all you need," the researchers used a cosine scheduler which periodically brings the learning rate up and down. Use whatever scheduler works best.

The last thing I want to touch on is the importance of a differentiable loss function. The loss function tells us how wrong the model is, but its derivative tells us what we can do about it. Simply returning a loss of 100 if the output doesn't match and 0 if it does will not work as you cannot differentiate that if statement, meaning the model will not be able to know what to do. The derivative informs how each parameter can improve to minimize the loss. This also leads to the fun fact that the magnitude of the loss does not affect what the model learns. If we use two different loss functions on the same model and they both return the same loss scalar, the two loss functions will still lead to that model learning different things. Remember, the gradients are what change the model during learning.

## The deeper we go

It has been shown time and time again that deeper models perform better than shallower models of the same number of parameters. Nevertheless, the limit for models in terms of layers is only on the order of hundreds, and you may even face difficulties training models with 20+ layers. The issue is with gradient flow. To differentiate a nested function (also known as a composite function), we use the chain rule:
$$
\frac{d}{dx}f(g(x)) = f'(g(x))\times g'(x)
$$
This can get ugly fast with 3 functions:
$$
\frac{d}{dx}f(g(h(x))) = f'(g(h(x)))\times g'(h(x))\times h'(x)
$$
But the commonality is that each function (which is a layer in the model) adds a new term we are multiplying with. If the derivative of each function is smaller than 1, the overall gradient vanishes to 0 and the model stops learning. If greater than 1, the gradient explodes to infinity and the model will struggle to take sensible steps. Large gradients can be clipped and down-scaled, but that is often just a band-aid; Small gradients need more thought.

A deeper model biases the model towards processing and transforming the input over a long duration before finally coming to a conclusion. While that might be sensible, it can be quite misleading as well. There are often partial answers that you can come up with on the way to the answer, and there might be entire lines of thinking that you skip simply because you've determined that they are not relevant. To achieve this in neural networks, you would segment contiguous layers into *residual blocks*. These blocks add their inputs directly to their final outputs, and this allows gradients to flow better as now they don't need to pass through all layers in the average case. It's an odd technique, but it is now an essential component in very deep models and it allows these blocks to learn themselves out of the model. If the output of the block before adding the input is close to 0, that block has determined that it doesn't have anything meaningful to add to the model.

That advice is of course only useful if you are writing very deep models, which you typically won't do. Though, there is one instance where you might accidentally do so, and that is with time-series layers. If you wanted to make a model that processed a whole sequence at once you could use Conv1D, but then your model is fundamentally limited by the exact sequence length. If the sequence is longer, your model wouldn't have enough layers of enough size to handle it, and if the sequence is shorter there won't be enough data to compute an answer. What if you could train a model to output an intermediate representation of what it has seen so far &mdash; a sort of summary of the history that only it can understand. You can then feed this data back into the model with the next item in the sequence allowing it to refer to the work it just did earlier. You can cut down on the model size as it will only ever process one input at a time, but it has this summary that it keeps track of. Even better, our model can handle a wider range of sequence lengths.

What I've just described is a recurrent neural network (RNN), but they really are a curse in disguise. To train such a model, in each batch we give entire sequences to the model and let it feed forward this intermediate data to itself. In the end, we compare the sequences of outputs to the expected sequences of outputs (sometimes we just compare the last value) and find the gradients. However, each successive output depends on the intermediate output of the previous layer, which depends on the previous layer and so on. We've just ran into the issue of depth yet again, and this time residuals can't help us as a residual over the whole model would be meaningless.

What we need are layers that learn how to forget. Such layers are Long-Short Term Memory Gates (LSTMs), and Gated Recurrent Units (GRUs). These layers function similarly to RNNs but with the addition of this forgetfulness they can modulate the gradients such that they maintain at a manageable magnitude. As such, these are essential in time-series analysis, but they have recently been dethroned by transformer models which use self-attention as an alternative way to handle a sequence. That is out of the scope of this article but 3 Blue 1 Brown has an excellent video series about these models.

## Normalizing Normalization

Statistical models only function well when given inputs that they are prepared for. A neural network is nothing more than a fancy statistical model, so it is imperative that we normalize the data we give it. Giving unbounded data is a recipe for disaster if you are not wary. At the bare minimum, the mean of the input data should be 0. If not, the range of the input data should involve 0 somewhere. In the case of images, we normalize the brightness of each color channel to $[0, 1]$ instead of $[0, 255]$, but $[-0.5, 0.5]$ is another option. In fact, the optimal solution is to find the mean brightness of your whole dataset and offset the range such that it is at 0. For example, if the mean is 0.33, the normalized range should be $[-0.33, 0.66]$.

Normalizing helps to keep the gradients at manageable levels as well. It's another way to bias the model towards worrying about relative differences and not absolute. If we normalize to 255, we are giving too much focus on this value of 255, when just the number 1 would do. It also makes sense to normalize between layers; After all, each layer is itself a statistical model. There are several normalization techniques, but the one that is often used for images is Batch Normalization. This technique scales and shifts the input data such that its mean is 0 and the standard of deviation is 1. It doesn't learn this and instead figures this out on the fly by keeping track of the running mean and variance of some number of previous inputs. Optionally (but is often used), it can learn what it should then shift the mean and standard of deviation to for the next layer. Because it adds this shift, the previous layer doesn't need its bias term anymore as the Batch Normalizer learns a "bias" as well. This is automatically handled in `gym-burn`.

The theory around Batch Normalization is a deep rabbit hole. It was intended to solve the problem of internal covariate shift and it did lead to much faster and more stable learning. However, it was later found that it actually doesn't solve that problem. As such, it's a little bit of a mystery why it helps models perform so well, and at least one person I found online avoids it because they don't like that no one fully understands it.

## A glance at Loss Functions

While it is certainly possible to write your own loss functions, the theory behind them is difficult. You might be better off using the pre-existing ones as they are proven to work well. The most common ones are Mean Squared Error and Binary Cross Entropy. The former is used for regression, and the latter is for classification between two categories. Cross Entropy is a generalization of Binary Cross Entropy and allows for training for classification between more than two categories *if* only one category is correct. Binary Cross Entropy can be used on multiple categories if more than one answer is acceptable. Mean Squared Error accepts any real number as input and expected, but the cross entropies only accept values in between 0 and 1 inclusive for each category, where 1 means yes and 0 means no. In our digit classification problem, we would use Cross Entropy as an image of a digit can only belong to one category of digit from 0 to 9.

It is also possible to sum the loss from different loss functions to optimize for multiple things. In Variational Auto-Encoders, we may use Binary Cross Entropy for the reconstruction loss added with the Kullback-Leibler Divergence loss to optimize for a more structured latent space. VAEs are a fascinating topic and are one of the simplest generative models. If you train a VAE on images of different pants, you can then use it to interpolate between pants. If you give it jeans and khakis, you can produce a video of all the pants that lie in between.

## Conclusion

If it's not clear by now, the answer to what design choices you should make is to just test it out. Do some quick research to see how other models are designed for similar tasks, copy, tweak, and train. Now go out and experiment.
